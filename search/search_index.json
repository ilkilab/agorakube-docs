{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Agorakube is an Opensource Kubernetes Distribution wich is aimed to provide the simplest way to install kubernetes on bare-metal, virtual & Cloud environments. Since November 2019, Agorakube has been certified by the \"Kubernetes Conformance Program\" and is a project of the cncf landscape . Agorakube install Hyper Customizable Kubernetes Clusters using native K8S binaries. Based on Infrastructure As Code (IaC) Ansible playbooks and roles, Agorakube is very easy to understand, especially for SysOps guys. Currently, Ubuntu 18.04, 20.04 amd64, Centos 7 and Debian 10 are supported, but several other operating systems will be available soon. Feel free to contribute.","title":"Agorakube"},{"location":"about/contributing/","text":"How to contribute \u00b6 We are really glad you're reading this, because we need volunteer developers to help this project come to fruition. We want you working on things you're excited about. You can contact us by mail agorakube@ilki.fr. Or you can join Agorakube's community for discussion and ask questions : Agorakube's Slack Channels : #general - For general purpose (news, events...) #developpers - For people who contribute to Agorakube by developing features #end-users - For end users who want to give us feedbacks #random - As its name suggests, for random discussions :) Set up a local test lab \u00b6 You can set a local test lab for Agorakube using Vagrant and VirtualBox. Testing \u00b6 Tests are not automated yet. Sample needs \u00b6 All contributions are welcome :) Here is a non-exhaustive list of contributions needed for this project: Documentation Add features like new runtime support, new Ingress Controller installation, new CNI plugin, etc... Improve the code quality ... Submitting changes \u00b6 Please send a GitHub Pull Request to AgoraKube with a clear list of what you've done (read more about pull requests ). When you send a pull request, we will love you forever if your code stay idempotent. We can always use more test coverage. Please follow our coding conventions (below) and make sure all of your commits are atomic (one feature per commit). Always write a clear log message for your commits. One-line messages are fine for small changes, but bigger changes should look like this: $ git commit -m \"A brief summary of the commit > > A paragraph describing what changed and its impact.\" Coding conventions \u00b6 Start reading our code and you'll get the hang of it. We optimize for readability: We indent using two spaces (soft tabs) We use task's name as comment in Ansible Playbooks. All names have to explicit the task goal. This is open source software. Consider the people who will read your code, and make it look nice for them. It's sort of like driving a car: Perhaps you love doing donuts when you're alone, but with passengers the goal is to make the ride as smooth as possible. You can use common tools like \"VisualStudioCode\", or \"Atom\" to make your Ansible code ! Thanks, Ilki team","title":"Contributing"},{"location":"about/contributing/#how-to-contribute","text":"We are really glad you're reading this, because we need volunteer developers to help this project come to fruition. We want you working on things you're excited about. You can contact us by mail agorakube@ilki.fr. Or you can join Agorakube's community for discussion and ask questions : Agorakube's Slack Channels : #general - For general purpose (news, events...) #developpers - For people who contribute to Agorakube by developing features #end-users - For end users who want to give us feedbacks #random - As its name suggests, for random discussions :)","title":"How to contribute"},{"location":"about/contributing/#set-up-a-local-test-lab","text":"You can set a local test lab for Agorakube using Vagrant and VirtualBox.","title":"Set up a local test lab"},{"location":"about/contributing/#testing","text":"Tests are not automated yet.","title":"Testing"},{"location":"about/contributing/#sample-needs","text":"All contributions are welcome :) Here is a non-exhaustive list of contributions needed for this project: Documentation Add features like new runtime support, new Ingress Controller installation, new CNI plugin, etc... Improve the code quality ...","title":"Sample needs"},{"location":"about/contributing/#submitting-changes","text":"Please send a GitHub Pull Request to AgoraKube with a clear list of what you've done (read more about pull requests ). When you send a pull request, we will love you forever if your code stay idempotent. We can always use more test coverage. Please follow our coding conventions (below) and make sure all of your commits are atomic (one feature per commit). Always write a clear log message for your commits. One-line messages are fine for small changes, but bigger changes should look like this: $ git commit -m \"A brief summary of the commit > > A paragraph describing what changed and its impact.\"","title":"Submitting changes"},{"location":"about/contributing/#coding-conventions","text":"Start reading our code and you'll get the hang of it. We optimize for readability: We indent using two spaces (soft tabs) We use task's name as comment in Ansible Playbooks. All names have to explicit the task goal. This is open source software. Consider the people who will read your code, and make it look nice for them. It's sort of like driving a car: Perhaps you love doing donuts when you're alone, but with passengers the goal is to make the ride as smooth as possible. You can use common tools like \"VisualStudioCode\", or \"Atom\" to make your Ansible code ! Thanks, Ilki team","title":"Coding conventions"},{"location":"about/license/","text":"All material here is released under the APACHE 2.0 license. All material that is not executable, including all text when not executed, is also released under the APACHE 2.0. In SPDX terms, everything here is licensed under APACHE 2.0; if it's not executable, including the text when extracted from code, it's \"(APACHE 2.0)\". Like almost all software today, this software depends on many other components with their own licenses. Not all components we depend on are APACHE 2.0-licensed, but all required components are FLOSS. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright 2019 ilki, and the other project contributors. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"main/configurations/","text":"Inventory file \u00b6 The first file to modify is \"./hosts\" . This file contains all architecture information about your K8S Cluster. All K8S servers names must be filled in by their FQDN. The next Sample deploys K8S components in HA mode on 6 nodes (3 etcd/masters nodes, 3 workers nodes and 3 storage nodes) : [deploy] master1 ansible_connection=local [masters] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [etcd] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [workers] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [storage] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [all:vars] advertise_ip_masters=10.10.20.3 # SSH connection settings ansible_ssh_extra_args='-o StrictHostKeyChecking=no' ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/ssh-private-key.pem The deploy section contains information about how to connect to the deployment machine. The etcd section contains information about the etcd machine(s) instances. The masters section contains information about the masters nodes (K8S Control Plane). The workers section contains information about the workers nodes (K8S Data Plane). The all:vars section contains information about how to connect to K8S nodes. The SSH Connection settings section contain information about the SSH connexion. You have to modify the variable ansible_ssh_private_key_file with the path where your public key is stored. ansible_user User used as service account by Agorakube to connect to all nodes. User must be sudoer . Configuration file \u00b6 The ../group_vars/all.yaml file contains all configuration variables that you can customize to make your K8S Cluster fit your needs. Sample file will deploy containerd as container runtime, flannel as CNI plugin and coredns as DNS service : --- # CERTIFICATES cn_root_ca: ilkilabs c: FR st: Ile-De-France l: Paris expiry: 87600h rotate_certs_pki: false rotate_full_pki: false # Components version etcd_release: v3.4.7 kubernetes_release: v1.18.2 delete_previous_k8s_install: False delete_etcd_install: False check_etcd_install: True # IPs-CIDR Configurations cluster_cidr: 10.33.0.0/16 service_cluster_ip_range: 10.32.0.0/24 kubernetes_service: 10.32.0.1 cluster_dns_ip: 10.32.0.10 service_node_port_range: 30000-32000 kube_proxy_mode: ipvs kube_proxy_ipvs_algotithm: rr cni_release: 0.8.5 # Custom features runtime: containerd network_cni_plugin: kube-router flannel_iface: default ingress_controller: nginx dns_server_soft: coredns populate_etc_hosts: yes k8s_dashboard: True service_mesh: none linkerd_release: stable-2.6.0 install_helm: False init_helm: False install_kubeapps: False # Calico calico_mtu: 1440 # Security encrypt_etcd_keys: # Warrning: If multiple keys are defined ONLY LAST KEY is used for encrypt and decrypt. # Other keys are used only for decrypt purpose key1: secret: 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= # Data Directory data_path: \"/var/agorakube\" etcd_data_directory: \"/var/lib/etcd\" #restoration_snapshot_file: /path/snopshot/file Located on {{ etcd_data_directory }} # KUBE-APISERVER spec kube_apiserver_enable_admission_plugins: # plugin AlwaysPullImage can be deleted. Credentials would be required to pull the private images every time. # Also, in trusted environments, this might increases load on network, registry, and decreases speed. # - AlwaysPullImages - NamespaceLifecycle # EventRateLimit is used to limit DoS on API server in case of event Flooding - EventRateLimit - LimitRanger - ServiceAccount - TaintNodesByCondition - PodNodeSelector - Priority - DefaultTolerationSeconds - DefaultStorageClass - StorageObjectInUseProtection - PersistentVolumeClaimResize - MutatingAdmissionWebhook - NodeRestriction - ValidatingAdmissionWebhook - RuntimeClass - ResourceQuota # SecurityContextDeny should be replaced by PodSecurityPolicy # - SecurityContextDeny # Rook Settings enable_rook: True rook_dataDirHostPath: /data/rook # Minio Settings # Rook MUST be enabed. enable_rook_minio: True rook_minio_infra_access_key: admin rook_minio_infra_secret_key: password # Monitoring. Rook MUST be enabled to use monitoring (Monitoring use StorageClass to persist data) enable_monitoring: False # Enable Harbor Registry - Contain Chartmuseum, notary, clair, registry. # Harbor will be expose by HTTPS with Ingress Resource. # Rook MUST be enabled to use Harbor (Harbor use StorageClass to persist data) install_harbor: False harbor_ingress_host: harbor.ilkilabs.io notary_ingress_host: notary.ilkilabs.io harbor_admin_password: ChangeMe! Note : You can also modify the IPs-CIDR if you want.","title":"Configuration"},{"location":"main/configurations/#inventory-file","text":"The first file to modify is \"./hosts\" . This file contains all architecture information about your K8S Cluster. All K8S servers names must be filled in by their FQDN. The next Sample deploys K8S components in HA mode on 6 nodes (3 etcd/masters nodes, 3 workers nodes and 3 storage nodes) : [deploy] master1 ansible_connection=local [masters] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [etcd] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [workers] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [storage] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [all:vars] advertise_ip_masters=10.10.20.3 # SSH connection settings ansible_ssh_extra_args='-o StrictHostKeyChecking=no' ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/ssh-private-key.pem The deploy section contains information about how to connect to the deployment machine. The etcd section contains information about the etcd machine(s) instances. The masters section contains information about the masters nodes (K8S Control Plane). The workers section contains information about the workers nodes (K8S Data Plane). The all:vars section contains information about how to connect to K8S nodes. The SSH Connection settings section contain information about the SSH connexion. You have to modify the variable ansible_ssh_private_key_file with the path where your public key is stored. ansible_user User used as service account by Agorakube to connect to all nodes. User must be sudoer .","title":"Inventory file"},{"location":"main/configurations/#configuration-file","text":"The ../group_vars/all.yaml file contains all configuration variables that you can customize to make your K8S Cluster fit your needs. Sample file will deploy containerd as container runtime, flannel as CNI plugin and coredns as DNS service : --- # CERTIFICATES cn_root_ca: ilkilabs c: FR st: Ile-De-France l: Paris expiry: 87600h rotate_certs_pki: false rotate_full_pki: false # Components version etcd_release: v3.4.7 kubernetes_release: v1.18.2 delete_previous_k8s_install: False delete_etcd_install: False check_etcd_install: True # IPs-CIDR Configurations cluster_cidr: 10.33.0.0/16 service_cluster_ip_range: 10.32.0.0/24 kubernetes_service: 10.32.0.1 cluster_dns_ip: 10.32.0.10 service_node_port_range: 30000-32000 kube_proxy_mode: ipvs kube_proxy_ipvs_algotithm: rr cni_release: 0.8.5 # Custom features runtime: containerd network_cni_plugin: kube-router flannel_iface: default ingress_controller: nginx dns_server_soft: coredns populate_etc_hosts: yes k8s_dashboard: True service_mesh: none linkerd_release: stable-2.6.0 install_helm: False init_helm: False install_kubeapps: False # Calico calico_mtu: 1440 # Security encrypt_etcd_keys: # Warrning: If multiple keys are defined ONLY LAST KEY is used for encrypt and decrypt. # Other keys are used only for decrypt purpose key1: secret: 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= # Data Directory data_path: \"/var/agorakube\" etcd_data_directory: \"/var/lib/etcd\" #restoration_snapshot_file: /path/snopshot/file Located on {{ etcd_data_directory }} # KUBE-APISERVER spec kube_apiserver_enable_admission_plugins: # plugin AlwaysPullImage can be deleted. Credentials would be required to pull the private images every time. # Also, in trusted environments, this might increases load on network, registry, and decreases speed. # - AlwaysPullImages - NamespaceLifecycle # EventRateLimit is used to limit DoS on API server in case of event Flooding - EventRateLimit - LimitRanger - ServiceAccount - TaintNodesByCondition - PodNodeSelector - Priority - DefaultTolerationSeconds - DefaultStorageClass - StorageObjectInUseProtection - PersistentVolumeClaimResize - MutatingAdmissionWebhook - NodeRestriction - ValidatingAdmissionWebhook - RuntimeClass - ResourceQuota # SecurityContextDeny should be replaced by PodSecurityPolicy # - SecurityContextDeny # Rook Settings enable_rook: True rook_dataDirHostPath: /data/rook # Minio Settings # Rook MUST be enabed. enable_rook_minio: True rook_minio_infra_access_key: admin rook_minio_infra_secret_key: password # Monitoring. Rook MUST be enabled to use monitoring (Monitoring use StorageClass to persist data) enable_monitoring: False # Enable Harbor Registry - Contain Chartmuseum, notary, clair, registry. # Harbor will be expose by HTTPS with Ingress Resource. # Rook MUST be enabled to use Harbor (Harbor use StorageClass to persist data) install_harbor: False harbor_ingress_host: harbor.ilkilabs.io notary_ingress_host: notary.ilkilabs.io harbor_admin_password: ChangeMe! Note : You can also modify the IPs-CIDR if you want.","title":"Configuration file"},{"location":"main/etcd_management/","text":"Backup etcd cluster \u00b6 You can backup ETCD cluster from Agorakube racine directory by following the next steps: # Run the following command: sudo ansible-playbook tools/etcd/backup-etcd-cluster.yaml Backup file will be saved on the deploy machine located in the following path: {{data_path}}/backups_etcd/ If you wish to customize the backup path, set the following variable in group_vars/all.yaml file: custom_etcd_backup_dir: /path/to/store/backups/on/deploy/machine Restore etcd cluster \u00b6 You can restore ETCD cluster from Agorakube racine directory by following the next steps: # Edit group_vars/all.yaml file and add the following variable: restoration_snapshot_file: /path/to/the/backups/file/on/deploy/machine # Then, from Agorakube racine directory, run the following command: sudo ansible-playbook tools/etcd/restore-etcd-cluster.yaml","title":"Manage ETCD"},{"location":"main/etcd_management/#backup-etcd-cluster","text":"You can backup ETCD cluster from Agorakube racine directory by following the next steps: # Run the following command: sudo ansible-playbook tools/etcd/backup-etcd-cluster.yaml Backup file will be saved on the deploy machine located in the following path: {{data_path}}/backups_etcd/ If you wish to customize the backup path, set the following variable in group_vars/all.yaml file: custom_etcd_backup_dir: /path/to/store/backups/on/deploy/machine","title":"Backup etcd cluster"},{"location":"main/etcd_management/#restore-etcd-cluster","text":"You can restore ETCD cluster from Agorakube racine directory by following the next steps: # Edit group_vars/all.yaml file and add the following variable: restoration_snapshot_file: /path/to/the/backups/file/on/deploy/machine # Then, from Agorakube racine directory, run the following command: sudo ansible-playbook tools/etcd/restore-etcd-cluster.yaml","title":"Restore etcd cluster"},{"location":"main/instructions/","text":"Table of Contents \u00b6 This is a list of points that will be explained in this instructions file for the AgoraKube project : High-level Architecture Prerequisites Nodes Setup K8S Cluster Configuration Agorakube Parameters Kubernetes deployment High-level Architecture \u00b6 Below a diagram of the high-level architecture deployed by AgoraKube : Notes : This distibution is aimed to be customizable so you can choose : Where the etcd will be deployed (with the master or not) The number of master nodes to deploy (from 1 to many) The number of etcd nodes to deploy (from 1 to many) The number of worker nodes to deploy (from 1 to many) The number of storage nodes to deploy (from 1 to many) Prerequisites \u00b6 This section explains what are the prerequisites to install AgoraKube in your environment. OS \u00b6 Below the OS currently supported on all the machines : Ubuntu 18.04 & 20.04 - amd64 Centos 7 & 8 - amd64 Debian 10 Node Sizing \u00b6 Below the sizing prerequisites for all the nodes (master and worker) : 2 GB or more of RAM per machine 2 CPUs or more Full network connectivity between all machines in the cluster (public or private network is fine) Full internet access Unique hostname, MAC address, and product_uuid for every node. See here for more details . Certain ports are open on your machines. See here for more details . Swap disabled. You MUST disable swap in order for the kubelet to work properly. Below the sizing prerequisites for the deployment machine : 2 GB or more of RAM 1 CPU or more Full network connectivity between all machines in the cluster (public or private network is fine) Full internet access Nodes Setup \u00b6 This section explains how to setup notes before deploying Kubernetes Clusters with AgoraKube. Deployment node \u00b6 The deployment node is an Ansible server which contains all Ansible roles and variables used to deploy and configure Kubernetes Clusters with AgoraKube distribution. Connect to the deployment node and run the following command : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-deploy.sh) K8S nodes \u00b6 The K8S nodes will host all the components needed for a Kubernetes cluster Control and Data planes. The prerequisites are: SSH Server (like Openssh) Python2 You can run the following command to automatically install those packages : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-hosts.sh) SSH keys creation \u00b6 Agorakube is using Ansible to deploy Kubernetes. You have to configure SSH keys to ensure the communication between the deploy machine and the others. On the deploy machine, create the SSH keys : ssh-keygen You can let everything by default. When your keys are created, you have to copy the public key in the other machine in the folder /home/yourUser/.ssh/authorized_keys, or you can use the following commands to copy the key : ssh-copy-id -i .ssh/id_rsa.pub yourUser@IP_OF_THE_HOST You have to execute this command for each node of your cluster Once your ssh keys have been pushed to all nodes, modify the file \"agorakube/hosts\" to add the user/ssh-key (in section SSH Connection settings ) that Agorakube will use to connect to all nodes K8S Cluster Configuration \u00b6 AgoraKube enables an easy way to deploy and manage customizable K8S clusters. Inventory file \u00b6 The first file to modify is \"./hosts\" . This file contains all architecture information about your K8S Cluster. All K8S servers names must be filled in by their FQDN. The next Sample deploys K8S components in HA mode on 6 nodes (3 etcd/masters nodes and 3 workers nodes) : [deploy] master1 ansible_connection=local [masters] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [etcd] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [workers] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [storage] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [all:vars] advertise_ip_masters=10.10.20.3 # SSH connection settings ansible_ssh_extra_args='-o StrictHostKeyChecking=no' ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/ssh-private-key.pem The deploy section contains information about how to connect to the deployment machine. The etcd section contains information about the etcd machine(s) instances. The masters section contains information about the masters nodes (K8S Control Plane). The workers section contains information about the workers nodes (K8S Data Plane). The storage section contains information about the storage nodes (K8S Storage Plane). The all:vars section contains information about how to connect to K8S nodes. The SSH Connection settings section contain information about the SSH connexion. You have to modify the variable ansible_ssh_private_key_file with the path where your public key is stored. ansible_user User used as service account by Agorakube to connect to all nodes. User must be sudoer . Configuration file \u00b6 The \"./group_vars/all.yaml\" file contains all configuration variables that you can customize to make your K8S Cluster fit your needs. Sample file will deploy containerd as container runtime, kube-router as CNI plugin and coredns as DNS service : --- # CERTIFICATES cn_root_ca: ilkilabs c: FR st: Ile-De-France l: Paris expiry: 87600h rotate_certs_pki: false rotate_full_pki: false # Components version etcd_release: v3.4.7 kubernetes_release: v1.18.2 delete_previous_k8s_install: False delete_etcd_install: False check_etcd_install: True # IPs-CIDR Configurations cluster_cidr: 10.33.0.0/16 service_cluster_ip_range: 10.32.0.0/24 kubernetes_service: 10.32.0.1 cluster_dns_ip: 10.32.0.10 service_node_port_range: 30000-32000 kube_proxy_mode: ipvs kube_proxy_ipvs_algotithm: rr cni_release: 0.8.5 # Custom features runtime: containerd network_cni_plugin: kube-router flannel_iface: default ingress_controller: nginx dns_server_soft: coredns populate_etc_hosts: yes k8s_dashboard: True service_mesh: none linkerd_release: stable-2.6.0 install_helm: False init_helm: False install_kubeapps: False # Calico calico_mtu: 1440 # Security encrypt_etcd_keys: # Warrning: If multiple keys are defined ONLY LAST KEY is used for encrypt and decrypt. # Other keys are used only for decrypt purpose key1: secret: 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= # Data Directory data_path: \"/var/agorakube\" etcd_data_directory: \"/var/lib/etcd\" #restoration_snapshot_file: /path/snopshot/file Located on {{ etcd_data_directory }} # KUBE-APISERVER spec kube_apiserver_enable_admission_plugins: # plugin AlwaysPullImage can be deleted. Credentials would be required to pull the private images every time. # Also, in trusted environments, this might increases load on network, registry, and decreases speed. # - AlwaysPullImages - NamespaceLifecycle # EventRateLimit is used to limit DoS on API server in case of event Flooding - EventRateLimit - LimitRanger - ServiceAccount - TaintNodesByCondition - PodNodeSelector - Priority - DefaultTolerationSeconds - DefaultStorageClass - StorageObjectInUseProtection - PersistentVolumeClaimResize - MutatingAdmissionWebhook - NodeRestriction - ValidatingAdmissionWebhook - RuntimeClass - ResourceQuota # SecurityContextDeny should be replaced by PodSecurityPolicy # - SecurityContextDeny # Rook Settings enable_rook: True rook_dataDirHostPath: /data/rook # Minio Settings # Rook MUST be enabed. enable_rook_minio: True rook_minio_infra_access_key: admin rook_minio_infra_secret_key: password # Monitoring. Rook MUST be enabled to use monitoring (Monitoring use StorageClass to persist data) enable_monitoring: False # Enable Harbor Registry - Contain Chartmuseum, notary, clair, registry. # Harbor will be expose by HTTPS with Ingress Resource. # Rook MUST be enabled to use Harbor (Harbor use StorageClass to persist data) install_harbor: False harbor_ingress_host: harbor.ilkilabs.io notary_ingress_host: notary.ilkilabs.io harbor_admin_password: ChangeMe! Note : You can also modify the IPs-CIDR if you want. Kubernetes deployment \u00b6 Once all configuration files are set, run the following command to launch the Ansible playbook that will deploy the pre-configured Kubernetes cluster : sudo ansible-playbook agorakube.yaml","title":"Instructions"},{"location":"main/instructions/#table-of-contents","text":"This is a list of points that will be explained in this instructions file for the AgoraKube project : High-level Architecture Prerequisites Nodes Setup K8S Cluster Configuration Agorakube Parameters Kubernetes deployment","title":"Table of Contents"},{"location":"main/instructions/#high-level-architecture","text":"Below a diagram of the high-level architecture deployed by AgoraKube : Notes : This distibution is aimed to be customizable so you can choose : Where the etcd will be deployed (with the master or not) The number of master nodes to deploy (from 1 to many) The number of etcd nodes to deploy (from 1 to many) The number of worker nodes to deploy (from 1 to many) The number of storage nodes to deploy (from 1 to many)","title":"High-level Architecture"},{"location":"main/instructions/#prerequisites","text":"This section explains what are the prerequisites to install AgoraKube in your environment.","title":"Prerequisites"},{"location":"main/instructions/#os","text":"Below the OS currently supported on all the machines : Ubuntu 18.04 & 20.04 - amd64 Centos 7 & 8 - amd64 Debian 10","title":"OS"},{"location":"main/instructions/#node-sizing","text":"Below the sizing prerequisites for all the nodes (master and worker) : 2 GB or more of RAM per machine 2 CPUs or more Full network connectivity between all machines in the cluster (public or private network is fine) Full internet access Unique hostname, MAC address, and product_uuid for every node. See here for more details . Certain ports are open on your machines. See here for more details . Swap disabled. You MUST disable swap in order for the kubelet to work properly. Below the sizing prerequisites for the deployment machine : 2 GB or more of RAM 1 CPU or more Full network connectivity between all machines in the cluster (public or private network is fine) Full internet access","title":"Node Sizing"},{"location":"main/instructions/#nodes-setup","text":"This section explains how to setup notes before deploying Kubernetes Clusters with AgoraKube.","title":"Nodes Setup"},{"location":"main/instructions/#deployment-node","text":"The deployment node is an Ansible server which contains all Ansible roles and variables used to deploy and configure Kubernetes Clusters with AgoraKube distribution. Connect to the deployment node and run the following command : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-deploy.sh)","title":"Deployment node"},{"location":"main/instructions/#k8s-nodes","text":"The K8S nodes will host all the components needed for a Kubernetes cluster Control and Data planes. The prerequisites are: SSH Server (like Openssh) Python2 You can run the following command to automatically install those packages : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-hosts.sh)","title":"K8S nodes"},{"location":"main/instructions/#ssh-keys-creation","text":"Agorakube is using Ansible to deploy Kubernetes. You have to configure SSH keys to ensure the communication between the deploy machine and the others. On the deploy machine, create the SSH keys : ssh-keygen You can let everything by default. When your keys are created, you have to copy the public key in the other machine in the folder /home/yourUser/.ssh/authorized_keys, or you can use the following commands to copy the key : ssh-copy-id -i .ssh/id_rsa.pub yourUser@IP_OF_THE_HOST You have to execute this command for each node of your cluster Once your ssh keys have been pushed to all nodes, modify the file \"agorakube/hosts\" to add the user/ssh-key (in section SSH Connection settings ) that Agorakube will use to connect to all nodes","title":"SSH keys creation"},{"location":"main/instructions/#k8s-cluster-configuration","text":"AgoraKube enables an easy way to deploy and manage customizable K8S clusters.","title":"K8S Cluster Configuration"},{"location":"main/instructions/#inventory-file","text":"The first file to modify is \"./hosts\" . This file contains all architecture information about your K8S Cluster. All K8S servers names must be filled in by their FQDN. The next Sample deploys K8S components in HA mode on 6 nodes (3 etcd/masters nodes and 3 workers nodes) : [deploy] master1 ansible_connection=local [masters] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [etcd] master1 ansible_host=10.10.20.3 master2 ansible_host=10.10.20.13 master3 ansible_host=10.10.20.23 [workers] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [storage] worker1 ansible_host=10.10.20.4 worker2 ansible_host=10.10.20.5 worker3 ansible_host=10.10.20.6 [all:vars] advertise_ip_masters=10.10.20.3 # SSH connection settings ansible_ssh_extra_args='-o StrictHostKeyChecking=no' ansible_user=vagrant ansible_ssh_private_key_file=/home/vagrant/ssh-private-key.pem The deploy section contains information about how to connect to the deployment machine. The etcd section contains information about the etcd machine(s) instances. The masters section contains information about the masters nodes (K8S Control Plane). The workers section contains information about the workers nodes (K8S Data Plane). The storage section contains information about the storage nodes (K8S Storage Plane). The all:vars section contains information about how to connect to K8S nodes. The SSH Connection settings section contain information about the SSH connexion. You have to modify the variable ansible_ssh_private_key_file with the path where your public key is stored. ansible_user User used as service account by Agorakube to connect to all nodes. User must be sudoer .","title":"Inventory file"},{"location":"main/instructions/#configuration-file","text":"The \"./group_vars/all.yaml\" file contains all configuration variables that you can customize to make your K8S Cluster fit your needs. Sample file will deploy containerd as container runtime, kube-router as CNI plugin and coredns as DNS service : --- # CERTIFICATES cn_root_ca: ilkilabs c: FR st: Ile-De-France l: Paris expiry: 87600h rotate_certs_pki: false rotate_full_pki: false # Components version etcd_release: v3.4.7 kubernetes_release: v1.18.2 delete_previous_k8s_install: False delete_etcd_install: False check_etcd_install: True # IPs-CIDR Configurations cluster_cidr: 10.33.0.0/16 service_cluster_ip_range: 10.32.0.0/24 kubernetes_service: 10.32.0.1 cluster_dns_ip: 10.32.0.10 service_node_port_range: 30000-32000 kube_proxy_mode: ipvs kube_proxy_ipvs_algotithm: rr cni_release: 0.8.5 # Custom features runtime: containerd network_cni_plugin: kube-router flannel_iface: default ingress_controller: nginx dns_server_soft: coredns populate_etc_hosts: yes k8s_dashboard: True service_mesh: none linkerd_release: stable-2.6.0 install_helm: False init_helm: False install_kubeapps: False # Calico calico_mtu: 1440 # Security encrypt_etcd_keys: # Warrning: If multiple keys are defined ONLY LAST KEY is used for encrypt and decrypt. # Other keys are used only for decrypt purpose key1: secret: 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= # Data Directory data_path: \"/var/agorakube\" etcd_data_directory: \"/var/lib/etcd\" #restoration_snapshot_file: /path/snopshot/file Located on {{ etcd_data_directory }} # KUBE-APISERVER spec kube_apiserver_enable_admission_plugins: # plugin AlwaysPullImage can be deleted. Credentials would be required to pull the private images every time. # Also, in trusted environments, this might increases load on network, registry, and decreases speed. # - AlwaysPullImages - NamespaceLifecycle # EventRateLimit is used to limit DoS on API server in case of event Flooding - EventRateLimit - LimitRanger - ServiceAccount - TaintNodesByCondition - PodNodeSelector - Priority - DefaultTolerationSeconds - DefaultStorageClass - StorageObjectInUseProtection - PersistentVolumeClaimResize - MutatingAdmissionWebhook - NodeRestriction - ValidatingAdmissionWebhook - RuntimeClass - ResourceQuota # SecurityContextDeny should be replaced by PodSecurityPolicy # - SecurityContextDeny # Rook Settings enable_rook: True rook_dataDirHostPath: /data/rook # Minio Settings # Rook MUST be enabed. enable_rook_minio: True rook_minio_infra_access_key: admin rook_minio_infra_secret_key: password # Monitoring. Rook MUST be enabled to use monitoring (Monitoring use StorageClass to persist data) enable_monitoring: False # Enable Harbor Registry - Contain Chartmuseum, notary, clair, registry. # Harbor will be expose by HTTPS with Ingress Resource. # Rook MUST be enabled to use Harbor (Harbor use StorageClass to persist data) install_harbor: False harbor_ingress_host: harbor.ilkilabs.io notary_ingress_host: notary.ilkilabs.io harbor_admin_password: ChangeMe! Note : You can also modify the IPs-CIDR if you want.","title":"Configuration file"},{"location":"main/instructions/#kubernetes-deployment","text":"Once all configuration files are set, run the following command to launch the Ansible playbook that will deploy the pre-configured Kubernetes cluster : sudo ansible-playbook agorakube.yaml","title":"Kubernetes deployment"},{"location":"main/main/","text":"What is AgoraKube \u00b6 AgoraKube is an easy-to-use, stable Kubernetes distribution (Kubernetes v1.15, 1.16, 1.17, 1.18, v1.19). By its symplicity, AgoraKube provide a good way to deploy and manage K8S Clusters. AgoraKube is based on Ansible scripts that install and configure Kubernetes components (control plane and data plane) quickly on bare-metal / VMs / Cloud Instances, as systemd services. This distribution is also adaptive by offering the opportunity to customize your deployment and fit to your needs : OS : Ubuntu-18.04/20.04-amd64 , Centos 7.X-amd64 and Debian 10 DNS Service: CoreDNS Ingress Controller Traefik (Default) & HA-Proxy & Nginx Container Runtime: Containerd (Default) & Docker Certificats: Self Signed PKI Service-Mesh: available: Linkerd Storage: Rook Ceph Block with StorageClass, and MinIO for Object Storage Registry: Harbor full featured Monitoring: Prometheus/Grafana CNI plugin: Flannel, Calico, Kube-router Packaging: Helm Self service application portal: Kubeapps ... This project is currently under active development so other customizable options will be added soon. How to install \u00b6 We regularly use a machine to deploy every cluster. We only use it for deployment. Setup \u00b6 On the \"deployment\" node \u00b6 Execute this command in order to install Ansible and clone the repository : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-deploy.sh) On the K8S nodes \u00b6 Execute this command on each node to update them and install the last version of Python : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-hosts.sh) Installation instructions \u00b6 To deploy your K8S cluster follow these instructions . How to give feedback \u00b6 Every feedback is very welcome via the GitHub site as issues or pull (merge) requests. You can also give use vulnerability reports by this way. How to contribute \u00b6 See our Code Of Conduct and CONTRIBUTING for more information. Community \u00b6 Join Agorakube's community for discussion and ask questions : Agorakube's Slack #random - As its name suggests, for random discussions :) Licensing \u00b6 All material here is released under the APACHE 2.0 license . All material that is not executable, including all text when not executed, is also released under the APACHE 2.0. In SPDX terms, everything here is licensed under APACHE 2.0; if it's not executable, including the text when extracted from code, it's \"(APACHE 2.0)\". Like almost all software today, this software depends on many other components with their own licenses. Not all components we depend on are APACHE 2.0-licensed, but all required components are FLOSS.","title":"Quickstart"},{"location":"main/main/#what-is-agorakube","text":"AgoraKube is an easy-to-use, stable Kubernetes distribution (Kubernetes v1.15, 1.16, 1.17, 1.18, v1.19). By its symplicity, AgoraKube provide a good way to deploy and manage K8S Clusters. AgoraKube is based on Ansible scripts that install and configure Kubernetes components (control plane and data plane) quickly on bare-metal / VMs / Cloud Instances, as systemd services. This distribution is also adaptive by offering the opportunity to customize your deployment and fit to your needs : OS : Ubuntu-18.04/20.04-amd64 , Centos 7.X-amd64 and Debian 10 DNS Service: CoreDNS Ingress Controller Traefik (Default) & HA-Proxy & Nginx Container Runtime: Containerd (Default) & Docker Certificats: Self Signed PKI Service-Mesh: available: Linkerd Storage: Rook Ceph Block with StorageClass, and MinIO for Object Storage Registry: Harbor full featured Monitoring: Prometheus/Grafana CNI plugin: Flannel, Calico, Kube-router Packaging: Helm Self service application portal: Kubeapps ... This project is currently under active development so other customizable options will be added soon.","title":"What is AgoraKube"},{"location":"main/main/#how-to-install","text":"We regularly use a machine to deploy every cluster. We only use it for deployment.","title":"How to install"},{"location":"main/main/#setup","text":"","title":"Setup"},{"location":"main/main/#on-the-deployment-node","text":"Execute this command in order to install Ansible and clone the repository : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-deploy.sh)","title":"On the \"deployment\" node"},{"location":"main/main/#on-the-k8s-nodes","text":"Execute this command on each node to update them and install the last version of Python : bash <(curl -s https://raw.githubusercontent.com/ilkilab/agorakube/master/setup-hosts.sh)","title":"On the K8S nodes"},{"location":"main/main/#installation-instructions","text":"To deploy your K8S cluster follow these instructions .","title":"Installation instructions"},{"location":"main/main/#how-to-give-feedback","text":"Every feedback is very welcome via the GitHub site as issues or pull (merge) requests. You can also give use vulnerability reports by this way.","title":"How to give feedback"},{"location":"main/main/#how-to-contribute","text":"See our Code Of Conduct and CONTRIBUTING for more information.","title":"How to contribute"},{"location":"main/main/#community","text":"Join Agorakube's community for discussion and ask questions : Agorakube's Slack #random - As its name suggests, for random discussions :)","title":"Community"},{"location":"main/main/#licensing","text":"All material here is released under the APACHE 2.0 license . All material that is not executable, including all text when not executed, is also released under the APACHE 2.0. In SPDX terms, everything here is licensed under APACHE 2.0; if it's not executable, including the text when extracted from code, it's \"(APACHE 2.0)\". Like almost all software today, this software depends on many other components with their own licenses. Not all components we depend on are APACHE 2.0-licensed, but all required components are FLOSS.","title":"Licensing"},{"location":"main/parameters/","text":"Agorakube Parameters \u00b6 Below you can find all the parameters you can use in this file, section by section. Certificates section \u00b6 This section is used to custom certificates information. Parameter Description Values cn_root_ca Certificate authority name Depend on your deployment ilkilabs (default) c Country where the certificate is issued Depend on your deployment FR (default) st State where the certificate is issued Depend on your deployment Ile-de-France (default) l City where the certificate is issued Depend on your deployment Paris (default) expiry Certificate lifetime in hours Depend on your needs 87600h (default) rotate_full_pki Update all the PKI (crts, keys and crs) of your cluster. You will need to regenerate manually your Service Account Tokens, and relaunch all pods that are using them false (default) true rotate_certs_pki Rotate certificates for your cluster false (default) true ### Components version section This section is used to custom the components version of your deployment. Parameter Description Values etcd_release Version of etcd component 3.3.X or 3.4.X 3.4.5 (default) kubernetes_release Version of kubernetes components 1.15.X , 1.16.X , 1.17.X or 1.18.X 1.18.0 (default) delete_previous_k8s_install Deletion of previous installations of Kubernetes true false (default) IPs-CIDR Configurations \u00b6 This section is used to custom network configurations of your deployment. Note : It will depend on the CNI plugin used. Parameter Description Values cluster_cidr CIDR used for all pods deployed in your cluster Depend on your deployment 10.33.0.0/16 (default) service_cluster_ip_range CIDR used for all services deployed in your cluster Depend on your deployment 10.32.0.0/16 (default) kubernetes_service IP used for Kubernetes service of your cluster. Must be the first IP of your service CIDR ! Depend on your deployment 10.32.0.1 (default) cluster_dns_ip IP used for DNS services deployed in your cluster Depend on your deployment 10.32.0.10 (default) service_node_port_range Range of ports used for all NodePort services deployed in your cluster depend on your deployment 30000-32767 (default) kube_proxy_mode Configure kube-proxy mode ipvs (default) iptables userspace kube_proxy_ipvs_algotithm Load Balancing algorithm for IPVS Kube-proxy mode rr (default - round-robin) lc (least connection) dh (destination hashing) sh (source hashing) sed (shortest expected delay) nq (never queue) Custom features section \u00b6 This section is used to defined all custom features of your deployment. Parameter Description Values runtime Container runtime used in your deployment containerd (default) docker network_cni_plugin CNI plugin used in your deployment calico flannel kube-router (default) flannel_iface Indicate to Flannel the specific iface to be binded default (default - take the first iface) Specific Iface ingress_controller Ingress Controller used in your deployment traefik (default) ha-proxy nginx none dns_server_soft DNS service used in your deployment CoreDNS (default) label_workers Fixed the label node-role.kubernetes.io/worker to all workers in your cluster false true (default) populate_etc_hosts Populate /etc/hosts file of all your nodes in the cluster no yes (default) k8s_dashboard Deploy Kubernetes dashboard in your cluster false true (default) service_mesh Service mesh used in your cluster none (default) linkerd linkerd_release Version of LinkerD used in your cluster stable-2.6.0 (default) none install_helm Helm installation in your cluster false (default) true init_helm Initialization of Helm false (default) true install_kubeapps Installation of Kubeapps - install_helm and init_helm have to be true also. false (default) true Other parameters sections \u00b6 Parameters for Calico CNI plugin : Parameter Description Values calico_mtu MTU size to used in your deployment Depend on your needs 1440 (default) Parameters for etcd : Parameter Description Values encrypt_etcd_keys Encryption keys used for etcd - Dictionary format Depend on your deployment 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= (default) check_etcd_install Display ETCD infos True (Default) False Parameters for Agorakube datas storage : Parameter Description Values data_path Path to Agorakube datas directory Depend on your deployment \"/var/agorakube\" (default) Parameters for etcd data location, and backups Parameter Description Values etcd_data_directory Directory to store etcd data on etcd members /var/lib/etcd/ (default) custom_etcd_backup_dir Directory where etcd leader backups are stored on deploy node {{data_path}}/backups_etcd/ (default if not defined) restoration_snapshot_file Path to the etcd snapshot on deploy node not defined (default) Rook Settings Parameter Description Values enable_rook Deploy Rook Ceph cluster on storage members False (default) True rook_dataDirHostPath Directory where Rook data are stored on Storage nodes /data/rook (default) enable_rook_minio Deploy Rook MinIO cluster on Rook Ceph Cluster False (default) True rook_minio_infra_access_key MinIO Admin Access Key admin_minio (default) rook_minio_infra_secret_key MinIO Admin Secret Key password_minio (default) Harbor Settings Parameter Description Values install_harbor Deploy Harbor Registry - Warrning : Rook Must be enabled ! False (default) true harbor_admin_password Admin password for Harbor UI ChangeMe! (default) harbor_ingress_host Host entry in Ingress. Harbor will be expose at https://{{ harbor_ingress_host }} (Depend on your ingress configuration) harbor.ilkilabs.io (default) notary_ingress_host Host entry in Ingress. Notary will be expose at https://{{ notary.ilkilabs.io }} (Depend on your ingress configuration) notary.ilkilabs.io (default) Monitoring Settings Parameter Description Values enable_monitoring Deploy monitoring - Warrning : Rook Must be enabled ! False (default) true Others settings: Parameter Description Values kube_apiserver_enable_admission_plugins List of admission plugins to be enabled Depend on your deployment","title":"Parameters"},{"location":"main/parameters/#agorakube-parameters","text":"Below you can find all the parameters you can use in this file, section by section.","title":"Agorakube Parameters"},{"location":"main/parameters/#certificates-section","text":"This section is used to custom certificates information. Parameter Description Values cn_root_ca Certificate authority name Depend on your deployment ilkilabs (default) c Country where the certificate is issued Depend on your deployment FR (default) st State where the certificate is issued Depend on your deployment Ile-de-France (default) l City where the certificate is issued Depend on your deployment Paris (default) expiry Certificate lifetime in hours Depend on your needs 87600h (default) rotate_full_pki Update all the PKI (crts, keys and crs) of your cluster. You will need to regenerate manually your Service Account Tokens, and relaunch all pods that are using them false (default) true rotate_certs_pki Rotate certificates for your cluster false (default) true ### Components version section This section is used to custom the components version of your deployment. Parameter Description Values etcd_release Version of etcd component 3.3.X or 3.4.X 3.4.5 (default) kubernetes_release Version of kubernetes components 1.15.X , 1.16.X , 1.17.X or 1.18.X 1.18.0 (default) delete_previous_k8s_install Deletion of previous installations of Kubernetes true false (default)","title":"Certificates section"},{"location":"main/parameters/#ips-cidr-configurations","text":"This section is used to custom network configurations of your deployment. Note : It will depend on the CNI plugin used. Parameter Description Values cluster_cidr CIDR used for all pods deployed in your cluster Depend on your deployment 10.33.0.0/16 (default) service_cluster_ip_range CIDR used for all services deployed in your cluster Depend on your deployment 10.32.0.0/16 (default) kubernetes_service IP used for Kubernetes service of your cluster. Must be the first IP of your service CIDR ! Depend on your deployment 10.32.0.1 (default) cluster_dns_ip IP used for DNS services deployed in your cluster Depend on your deployment 10.32.0.10 (default) service_node_port_range Range of ports used for all NodePort services deployed in your cluster depend on your deployment 30000-32767 (default) kube_proxy_mode Configure kube-proxy mode ipvs (default) iptables userspace kube_proxy_ipvs_algotithm Load Balancing algorithm for IPVS Kube-proxy mode rr (default - round-robin) lc (least connection) dh (destination hashing) sh (source hashing) sed (shortest expected delay) nq (never queue)","title":"IPs-CIDR Configurations"},{"location":"main/parameters/#custom-features-section","text":"This section is used to defined all custom features of your deployment. Parameter Description Values runtime Container runtime used in your deployment containerd (default) docker network_cni_plugin CNI plugin used in your deployment calico flannel kube-router (default) flannel_iface Indicate to Flannel the specific iface to be binded default (default - take the first iface) Specific Iface ingress_controller Ingress Controller used in your deployment traefik (default) ha-proxy nginx none dns_server_soft DNS service used in your deployment CoreDNS (default) label_workers Fixed the label node-role.kubernetes.io/worker to all workers in your cluster false true (default) populate_etc_hosts Populate /etc/hosts file of all your nodes in the cluster no yes (default) k8s_dashboard Deploy Kubernetes dashboard in your cluster false true (default) service_mesh Service mesh used in your cluster none (default) linkerd linkerd_release Version of LinkerD used in your cluster stable-2.6.0 (default) none install_helm Helm installation in your cluster false (default) true init_helm Initialization of Helm false (default) true install_kubeapps Installation of Kubeapps - install_helm and init_helm have to be true also. false (default) true","title":"Custom features section"},{"location":"main/parameters/#other-parameters-sections","text":"Parameters for Calico CNI plugin : Parameter Description Values calico_mtu MTU size to used in your deployment Depend on your needs 1440 (default) Parameters for etcd : Parameter Description Values encrypt_etcd_keys Encryption keys used for etcd - Dictionary format Depend on your deployment 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= (default) check_etcd_install Display ETCD infos True (Default) False Parameters for Agorakube datas storage : Parameter Description Values data_path Path to Agorakube datas directory Depend on your deployment \"/var/agorakube\" (default) Parameters for etcd data location, and backups Parameter Description Values etcd_data_directory Directory to store etcd data on etcd members /var/lib/etcd/ (default) custom_etcd_backup_dir Directory where etcd leader backups are stored on deploy node {{data_path}}/backups_etcd/ (default if not defined) restoration_snapshot_file Path to the etcd snapshot on deploy node not defined (default) Rook Settings Parameter Description Values enable_rook Deploy Rook Ceph cluster on storage members False (default) True rook_dataDirHostPath Directory where Rook data are stored on Storage nodes /data/rook (default) enable_rook_minio Deploy Rook MinIO cluster on Rook Ceph Cluster False (default) True rook_minio_infra_access_key MinIO Admin Access Key admin_minio (default) rook_minio_infra_secret_key MinIO Admin Secret Key password_minio (default) Harbor Settings Parameter Description Values install_harbor Deploy Harbor Registry - Warrning : Rook Must be enabled ! False (default) true harbor_admin_password Admin password for Harbor UI ChangeMe! (default) harbor_ingress_host Host entry in Ingress. Harbor will be expose at https://{{ harbor_ingress_host }} (Depend on your ingress configuration) harbor.ilkilabs.io (default) notary_ingress_host Host entry in Ingress. Notary will be expose at https://{{ notary.ilkilabs.io }} (Depend on your ingress configuration) notary.ilkilabs.io (default) Monitoring Settings Parameter Description Values enable_monitoring Deploy monitoring - Warrning : Rook Must be enabled ! False (default) true Others settings: Parameter Description Values kube_apiserver_enable_admission_plugins List of admission plugins to be enabled Depend on your deployment","title":"Other parameters sections"},{"location":"tutorials/deploy_agorakube/","text":"In this tutorial, we will use virtual machines that will be created by vagrant using VirtualBox. Vagrant is an orchestration tool for building and managing virtual machine environments in a single workflow. VirtualBox is an Hypervisor Type 2 and it is used to provision Virtual Machines localy. Deploy the environment \u00b6 If you want to deploy the same Kubernetes cluster as this tutorial you will need at least a host with 12GB of RAM, but you can adapt it. The cluster will be composed of 4 Virtual machines. 1 machine Deploy that will be used as a deployment machine. This machine is used to manage your K8S cluster. 1 machine master/etcd 2 machine worker/storage that are used as worker \"Note: If you don't have enough RAM for this tutorial, we suggest deploying an all-in-one cluster (1 machine with all roles). A Vagrantfile is provided for this in the project Agorakube in ./test_lab * Below you have the Vagrantfile used for this tutorial. We are using 4 machines Ubuntu 18.04. You are free to change the OS to CentOS 7 if you want. Create a file named \"Vagrantfile\" with the following content: # # Vagrantfile # Vagrant . configure ( \"2\" ) do | config | config . vm . box = \"bento/ubuntu-18.04\" config . vm . define \"deploy\" do | deploy | deploy . vm . hostname = \"deploy\" deploy . vm . network \"private_network\" , ip : \"10.0.0.10\" deploy . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"deploy\" end end config . vm . define \"master\" do | master | master . vm . hostname = \"master\" master . vm . network \"private_network\" , ip : \"10.0.0.11\" master . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"master\" end end config . vm . define \"worker1\" do | worker1 | worker1 . vm . hostname = \"worker1\" worker1 . vm . network \"private_network\" , ip : \"10.0.0.12\" worker1 . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"worker1\" end end config . vm . define \"worker2\" do | worker2 | worker2 . vm . hostname = \"worker2\" worker2 . vm . network \"private_network\" , ip : \"10.0.0.13\" worker2 . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"worker2\" end end end Installation of VMs used in this tutorial \u00b6 If you want to use the same method of deployment as this tutorial follow those steps. Install VirtualBox This is necessary to use a version of VirtualBox supported by Vagrant otherwise you will not have the possibility to use the Vagrantfile. Install Vagrant You can install the latest version Deploy the cluster with the Vagrantfile provided below To deploy with the Vagrantfile, create a file Vagrantfile (with no extension!) with the script below Open a command line in the path of this file Use the command vagrant up (it will start the installation of your machines) You can connect to your VMs using vagrant ssh NameOfTheMachine If you want to stop your cluster use the command vagrant halt -f If you want to delete a specific machine vagrant delete -f NameOfTheMachine If you want to delete your cluster vagrant delete -f \"Note: All machines have to be able to communicate to each other.\" The deploy machine has to be able to communicate by SSH with the others machines. Setup Agorakube \u00b6 Now that all your nodes are running, you will be able to setup Agorakube. 1) Connect to you \"deploy\" node using vagrant ssh deploy command and then run the following command: bash <(curl -s https://raw.githubusercontent.com/ilkilab/Agorakube/master/setup-deploy.sh) This command will install Ansible and clone the Agorakube repository on your current path. 2) Next, connect to other Virtual Machines and run the following command on all machines. bash <(curl -s https://raw.githubusercontent.com/ilkilab/Agorakube/master/setup-hosts.sh) This command will update the machine and install the latest version of Python and Openssh-server (Ansible needs Python and SSH to work). Create a pair of SSH keys \u00b6 Agorakube is using Ansible to deploy Kubernetes. Ansible needs an SSH connection to configure the other machines. For that you need to create SSH keys pair. Private key will stay on Deploy machine and public key will be deployed on all machines. In this part you will create and configure your ssh key pair. 1) Connect to your deploy machine and use the following command to create a SSH key pair. ssh-keygen It will create a Pair of keys that you will use for you SSH connection. Do not set SSH passphrase ! I suggest letting everything by default for this tutorial, so you will be able to copy and paste most of the code provided in this tutorial. By default ssh key pair is generated under \"/home/YOUR_USER/.ssh/ . Private key is \"id_rsa\" and public key is \"id_rsa.pub\". Once you have created your SSH key pair, do not forget to push the public key (here id_rsa.pub) in the remote machines in /home/YOUR_USER/.ssh/authorized_keys file. You can use the following command from \"deploy\" machine to push your SSH public key to other machines: ssh-copy-id vagrant@IP (Replace IP with the IP of the remote Host). This command will copy the SSH public key located in ~/.ssh/id_rsa.pub to the file /home/vagrant/.ssh/authorized_keys located on the remote host. The password for vagrant user is \"vagrant\". Modify the hosts file \u00b6 Now, you need to modify the ./hosts file of Agorakube. This file is composed of 6 parts: - deploy will provide the name of the deploy machine - master will provide a list of master machines to deploy (from 1 to many - best numbers are [1,3,5]) - etcd will provide a list of etcd machines to deploy (from 1 to many - best numbers are [1,3,5]) - worker will provide a list of worker machines to deploy (from 1 to many ) - storage will provide a list of storage machine to deploy (from 1 to many - best numbers are [1,3,5]) The last block [all:vars] is used to declare one IP that is used to publish K8S control plan. (In a production environment this IP is a LoadBalancer that announce K8S masters - port 6443). Other parametres in [all:vars] are used to specify how Agorakube/Ansible can connect to cluster machines. Below the file used for this tutorial. You can modify it with your own specifications. [deploy] deploy ansible_connection = local [masters] master ansible_host = 10.0.0.11 [etcd] master ansible_host = 10.0.0.11 [workers] worker1 ansible_host = 10.0.0.12 worker2 ansible_host = 10.0.0.13 [storage] worker1 ansible_host = 10.0.0.12 worker2 ansible_host = 10.0.0.13 [all:vars] advertise_ip_masters = 10.0.0.11 ansible_ssh_extra_args = '-o StrictHostKeyChecking=no' ansible_user = vagrant ansible_ssh_private_key_file = /home/vagrant/.ssh/id_rsa \"Note: When you declare a machine, you have to use the FQDN.\" Configure Agorakube Settings \u00b6 Agorakube is fully customizable. To customize it, modify ./group_vars/all.yaml file. You have all information about this file in the documentation . You have below the file used for this tutorial: --- # CERTIFICATES cn_root_ca: ilkilabs c: FR st: Ile-De-France l: Paris expiry: 87600h rotate_certs_pki: false rotate_full_pki: false # Components version etcd_release: v3.4.7 kubernetes_release: v1.18.3 delete_previous_k8s_install: False delete_etcd_install: False check_etcd_install: True # IPs-CIDR Configurations cluster_cidr: 10.33.0.0/16 service_cluster_ip_range: 10.32.0.0/24 kubernetes_service: 10.32.0.1 cluster_dns_ip: 10.32.0.10 service_node_port_range: 30000-32000 kube_proxy_mode: ipvs kube_proxy_ipvs_algotithm: rr cni_release: 0.8.5 # Custom features runtime: containerd network_cni_plugin: kube-router flannel_iface: default ingress_controller: traefik dns_server_soft: coredns populate_etc_hosts: yes k8s_dashboard: True service_mesh: none linkerd_release: stable-2.6.0 install_helm: False init_helm: False install_kubeapps: False # Calico calico_mtu: 1440 # Security encrypt_etcd_keys: # Warrning: If multiple keys are defined ONLY LAST KEY is used for encrypt and decrypt. # Other keys are used only for decrypt purpose key1: secret: 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= # Data Directory data_path: \"/var/agorakube\" etcd_data_directory: \"/var/lib/etcd\" #restoration_snapshot_file: /path/snopshot/file Located on {{ etcd_data_directory }} # KUBE-APISERVER spec kube_apiserver_enable_admission_plugins: # plugin AlwaysPullImage can be deleted. Credentials would be required to pull the private images every time. # Also, in trusted environments, this might increases load on network, registry, and decreases speed. # - AlwaysPullImages - NamespaceLifecycle # EventRateLimit is used to limit DoS on API server in case of event Flooding - EventRateLimit - LimitRanger - ServiceAccount - TaintNodesByCondition - PodNodeSelector - Priority - DefaultTolerationSeconds - DefaultStorageClass - StorageObjectInUseProtection - PersistentVolumeClaimResize - MutatingAdmissionWebhook - NodeRestriction - ValidatingAdmissionWebhook - RuntimeClass - ResourceQuota # SecurityContextDeny should be replaced by PodSecurityPolicy # - SecurityContextDeny # Rook Settings enable_rook: False rook_dataDirHostPath: /data/rook # Minio Settings # Minio Key MUST BE Base64 encoded and Rook MUST be enabed. enable_rook_minio: False rook_minio_infra_access_key: admin_minio rook_minio_infra_secret_key: password_minio # Monitoring. Rook MUST be enabled to use monitoring (Monitoring use StorageClass to persist data) enable_monitoring: False # Enable Harbor Registry - Contain Chartmuseum, notary, clair, registry. # Harbor will be expose by HTTPS with Ingress Resource. # Rook MUST be enabled to use Harbor (Harbor use StorageClass to persist data) install_harbor: False harbor_ingress_host: harbor.ilkilabs.io notary_ingress_host: notary.ilkilabs.io harbor_admin_password: ChangeMe! Deploy K8S with Agorakube \u00b6 Now that your K8S cluster is defined and configured in \".hosts\" and \"./groug_vars/all.yaml\" files you are ready to run the Agorakube installation. Run the followinf command: sudo ansible-playbook agorakube.yaml The installation prosses can take a wile to complite, so just wait. At the end of installation, you must see the following screen: Note: If some errors occures, juste run again the agorakube installation. Agorakube installation process is idempotent. Play with kubernetes \u00b6 Oncee Agorakube has been succefully installed you can use your kubernetes cluster to deploy and manage your apps. By default, \"kubectl\" is configured only for root user on deploy machine Connect to kubernetes \u00b6 1) Connect to your Deploy machine and then become root with the following command sudo su 2) Now, list all your nodes with the following command : kubectl get nodes -o wide 3) List all your pods with following command : kubectl get pods --all-namespaces -o wide 4) List all your K8S control plan services with : kubectl get cs Note: The best way to manage Kubernetes cluster is not from the deploy machine itself, but from a client machine (For exemple your computer/laptop). You should install kubectl on your computer and download the kubeconfig file located in \"deploy\" machine at \"/root/.kube/config\".","title":"Deploy Agorakube VirtualBox"},{"location":"tutorials/deploy_agorakube/#deploy-the-environment","text":"If you want to deploy the same Kubernetes cluster as this tutorial you will need at least a host with 12GB of RAM, but you can adapt it. The cluster will be composed of 4 Virtual machines. 1 machine Deploy that will be used as a deployment machine. This machine is used to manage your K8S cluster. 1 machine master/etcd 2 machine worker/storage that are used as worker \"Note: If you don't have enough RAM for this tutorial, we suggest deploying an all-in-one cluster (1 machine with all roles). A Vagrantfile is provided for this in the project Agorakube in ./test_lab * Below you have the Vagrantfile used for this tutorial. We are using 4 machines Ubuntu 18.04. You are free to change the OS to CentOS 7 if you want. Create a file named \"Vagrantfile\" with the following content: # # Vagrantfile # Vagrant . configure ( \"2\" ) do | config | config . vm . box = \"bento/ubuntu-18.04\" config . vm . define \"deploy\" do | deploy | deploy . vm . hostname = \"deploy\" deploy . vm . network \"private_network\" , ip : \"10.0.0.10\" deploy . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"deploy\" end end config . vm . define \"master\" do | master | master . vm . hostname = \"master\" master . vm . network \"private_network\" , ip : \"10.0.0.11\" master . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"master\" end end config . vm . define \"worker1\" do | worker1 | worker1 . vm . hostname = \"worker1\" worker1 . vm . network \"private_network\" , ip : \"10.0.0.12\" worker1 . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"worker1\" end end config . vm . define \"worker2\" do | worker2 | worker2 . vm . hostname = \"worker2\" worker2 . vm . network \"private_network\" , ip : \"10.0.0.13\" worker2 . vm . provider \"virtualbox\" do | v | v . memory = 2048 v . cpus = 1 v . name = \"worker2\" end end end","title":"Deploy the environment"},{"location":"tutorials/deploy_agorakube/#installation-of-vms-used-in-this-tutorial","text":"If you want to use the same method of deployment as this tutorial follow those steps. Install VirtualBox This is necessary to use a version of VirtualBox supported by Vagrant otherwise you will not have the possibility to use the Vagrantfile. Install Vagrant You can install the latest version Deploy the cluster with the Vagrantfile provided below To deploy with the Vagrantfile, create a file Vagrantfile (with no extension!) with the script below Open a command line in the path of this file Use the command vagrant up (it will start the installation of your machines) You can connect to your VMs using vagrant ssh NameOfTheMachine If you want to stop your cluster use the command vagrant halt -f If you want to delete a specific machine vagrant delete -f NameOfTheMachine If you want to delete your cluster vagrant delete -f \"Note: All machines have to be able to communicate to each other.\" The deploy machine has to be able to communicate by SSH with the others machines.","title":"Installation of VMs used in this tutorial"},{"location":"tutorials/deploy_agorakube/#setup-agorakube","text":"Now that all your nodes are running, you will be able to setup Agorakube. 1) Connect to you \"deploy\" node using vagrant ssh deploy command and then run the following command: bash <(curl -s https://raw.githubusercontent.com/ilkilab/Agorakube/master/setup-deploy.sh) This command will install Ansible and clone the Agorakube repository on your current path. 2) Next, connect to other Virtual Machines and run the following command on all machines. bash <(curl -s https://raw.githubusercontent.com/ilkilab/Agorakube/master/setup-hosts.sh) This command will update the machine and install the latest version of Python and Openssh-server (Ansible needs Python and SSH to work).","title":"Setup Agorakube"},{"location":"tutorials/deploy_agorakube/#create-a-pair-of-ssh-keys","text":"Agorakube is using Ansible to deploy Kubernetes. Ansible needs an SSH connection to configure the other machines. For that you need to create SSH keys pair. Private key will stay on Deploy machine and public key will be deployed on all machines. In this part you will create and configure your ssh key pair. 1) Connect to your deploy machine and use the following command to create a SSH key pair. ssh-keygen It will create a Pair of keys that you will use for you SSH connection. Do not set SSH passphrase ! I suggest letting everything by default for this tutorial, so you will be able to copy and paste most of the code provided in this tutorial. By default ssh key pair is generated under \"/home/YOUR_USER/.ssh/ . Private key is \"id_rsa\" and public key is \"id_rsa.pub\". Once you have created your SSH key pair, do not forget to push the public key (here id_rsa.pub) in the remote machines in /home/YOUR_USER/.ssh/authorized_keys file. You can use the following command from \"deploy\" machine to push your SSH public key to other machines: ssh-copy-id vagrant@IP (Replace IP with the IP of the remote Host). This command will copy the SSH public key located in ~/.ssh/id_rsa.pub to the file /home/vagrant/.ssh/authorized_keys located on the remote host. The password for vagrant user is \"vagrant\".","title":"Create a pair of SSH keys"},{"location":"tutorials/deploy_agorakube/#modify-the-hosts-file","text":"Now, you need to modify the ./hosts file of Agorakube. This file is composed of 6 parts: - deploy will provide the name of the deploy machine - master will provide a list of master machines to deploy (from 1 to many - best numbers are [1,3,5]) - etcd will provide a list of etcd machines to deploy (from 1 to many - best numbers are [1,3,5]) - worker will provide a list of worker machines to deploy (from 1 to many ) - storage will provide a list of storage machine to deploy (from 1 to many - best numbers are [1,3,5]) The last block [all:vars] is used to declare one IP that is used to publish K8S control plan. (In a production environment this IP is a LoadBalancer that announce K8S masters - port 6443). Other parametres in [all:vars] are used to specify how Agorakube/Ansible can connect to cluster machines. Below the file used for this tutorial. You can modify it with your own specifications. [deploy] deploy ansible_connection = local [masters] master ansible_host = 10.0.0.11 [etcd] master ansible_host = 10.0.0.11 [workers] worker1 ansible_host = 10.0.0.12 worker2 ansible_host = 10.0.0.13 [storage] worker1 ansible_host = 10.0.0.12 worker2 ansible_host = 10.0.0.13 [all:vars] advertise_ip_masters = 10.0.0.11 ansible_ssh_extra_args = '-o StrictHostKeyChecking=no' ansible_user = vagrant ansible_ssh_private_key_file = /home/vagrant/.ssh/id_rsa \"Note: When you declare a machine, you have to use the FQDN.\"","title":"Modify the hosts file"},{"location":"tutorials/deploy_agorakube/#configure-agorakube-settings","text":"Agorakube is fully customizable. To customize it, modify ./group_vars/all.yaml file. You have all information about this file in the documentation . You have below the file used for this tutorial: --- # CERTIFICATES cn_root_ca: ilkilabs c: FR st: Ile-De-France l: Paris expiry: 87600h rotate_certs_pki: false rotate_full_pki: false # Components version etcd_release: v3.4.7 kubernetes_release: v1.18.3 delete_previous_k8s_install: False delete_etcd_install: False check_etcd_install: True # IPs-CIDR Configurations cluster_cidr: 10.33.0.0/16 service_cluster_ip_range: 10.32.0.0/24 kubernetes_service: 10.32.0.1 cluster_dns_ip: 10.32.0.10 service_node_port_range: 30000-32000 kube_proxy_mode: ipvs kube_proxy_ipvs_algotithm: rr cni_release: 0.8.5 # Custom features runtime: containerd network_cni_plugin: kube-router flannel_iface: default ingress_controller: traefik dns_server_soft: coredns populate_etc_hosts: yes k8s_dashboard: True service_mesh: none linkerd_release: stable-2.6.0 install_helm: False init_helm: False install_kubeapps: False # Calico calico_mtu: 1440 # Security encrypt_etcd_keys: # Warrning: If multiple keys are defined ONLY LAST KEY is used for encrypt and decrypt. # Other keys are used only for decrypt purpose key1: secret: 1fJcKt6vBxMt+AkBanoaxFF2O6ytHIkETNgQWv4b/+Q= # Data Directory data_path: \"/var/agorakube\" etcd_data_directory: \"/var/lib/etcd\" #restoration_snapshot_file: /path/snopshot/file Located on {{ etcd_data_directory }} # KUBE-APISERVER spec kube_apiserver_enable_admission_plugins: # plugin AlwaysPullImage can be deleted. Credentials would be required to pull the private images every time. # Also, in trusted environments, this might increases load on network, registry, and decreases speed. # - AlwaysPullImages - NamespaceLifecycle # EventRateLimit is used to limit DoS on API server in case of event Flooding - EventRateLimit - LimitRanger - ServiceAccount - TaintNodesByCondition - PodNodeSelector - Priority - DefaultTolerationSeconds - DefaultStorageClass - StorageObjectInUseProtection - PersistentVolumeClaimResize - MutatingAdmissionWebhook - NodeRestriction - ValidatingAdmissionWebhook - RuntimeClass - ResourceQuota # SecurityContextDeny should be replaced by PodSecurityPolicy # - SecurityContextDeny # Rook Settings enable_rook: False rook_dataDirHostPath: /data/rook # Minio Settings # Minio Key MUST BE Base64 encoded and Rook MUST be enabed. enable_rook_minio: False rook_minio_infra_access_key: admin_minio rook_minio_infra_secret_key: password_minio # Monitoring. Rook MUST be enabled to use monitoring (Monitoring use StorageClass to persist data) enable_monitoring: False # Enable Harbor Registry - Contain Chartmuseum, notary, clair, registry. # Harbor will be expose by HTTPS with Ingress Resource. # Rook MUST be enabled to use Harbor (Harbor use StorageClass to persist data) install_harbor: False harbor_ingress_host: harbor.ilkilabs.io notary_ingress_host: notary.ilkilabs.io harbor_admin_password: ChangeMe!","title":"Configure Agorakube Settings"},{"location":"tutorials/deploy_agorakube/#deploy-k8s-with-agorakube","text":"Now that your K8S cluster is defined and configured in \".hosts\" and \"./groug_vars/all.yaml\" files you are ready to run the Agorakube installation. Run the followinf command: sudo ansible-playbook agorakube.yaml The installation prosses can take a wile to complite, so just wait. At the end of installation, you must see the following screen: Note: If some errors occures, juste run again the agorakube installation. Agorakube installation process is idempotent.","title":"Deploy K8S with Agorakube"},{"location":"tutorials/deploy_agorakube/#play-with-kubernetes","text":"Oncee Agorakube has been succefully installed you can use your kubernetes cluster to deploy and manage your apps. By default, \"kubectl\" is configured only for root user on deploy machine","title":"Play with kubernetes"},{"location":"tutorials/deploy_agorakube/#connect-to-kubernetes","text":"1) Connect to your Deploy machine and then become root with the following command sudo su 2) Now, list all your nodes with the following command : kubectl get nodes -o wide 3) List all your pods with following command : kubectl get pods --all-namespaces -o wide 4) List all your K8S control plan services with : kubectl get cs Note: The best way to manage Kubernetes cluster is not from the deploy machine itself, but from a client machine (For exemple your computer/laptop). You should install kubectl on your computer and download the kubeconfig file located in \"deploy\" machine at \"/root/.kube/config\".","title":"Connect to kubernetes"}]}